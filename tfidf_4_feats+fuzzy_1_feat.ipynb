{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import operator\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix,coo_matrix,csc_matrix\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from operator import itemgetter\n",
    "import scipy as sp\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install sparse dot topn\n",
    "\n",
    "#!pip3 install git+https://github.com/ing-bank/sparse_dot_topn.git\n",
    "if sys.version_info[0] >= 3:\n",
    "    from sparse_dot_topn import sparse_dot_topn as ct\n",
    "    from sparse_dot_topn import sparse_dot_topn_threaded as ct_thread\n",
    "else:\n",
    "    import sparse_dot_topn as ct\n",
    "    import sparse_dot_topn_threaded as ct_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def timeit(method):\n",
    "    \"\"\"\n",
    "    Standard Python decorator that measures the execution time of a method;\n",
    "    \"\"\"\n",
    "    def timed(*args, **kw):\n",
    "        start = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        end = time.time()\n",
    "        \n",
    "        #print(f\"{method.__name__}:  {(end - start):.2f} s\")\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "\n",
    "def read_file(path: str, set_record_id_as_index: bool=False) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, dtype=str, escapechar=\"\\\\\", index_col=\"record_id\" if set_record_id_as_index else None)\n",
    "\n",
    "\n",
    "#@timeit\n",
    "def load_training_data(data_path: str, row_subset: float=1, train_split: float=0.7, shuffle: bool=False, seed=None):\n",
    "    '''\n",
    "    Load the training set and divide it into training and test splits.\n",
    "    \"LinkedID\" is the value that we want to predict\n",
    "    :param data_path: path to the dataset to load;\n",
    "    :param row_subset: use only the specified fraction of rows in the dataset (value in (0, 1]);\n",
    "    :param train_split: fraction of rows placed in the training set;\n",
    "    :param shuffle: if True, shuffle the rows before splitting or subsetting the data;\n",
    "    '''\n",
    "    if row_subset <= 0 or row_subset > 1:\n",
    "        row_subset = 1\n",
    "    \n",
    "    data = read_file(training_file, set_record_id_as_index=True)\n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1, random_state=seed)\n",
    "    # Obtain the specified subset of rows;\n",
    "    data = data.iloc[:int(np.ceil(len(data) * row_subset))]\n",
    "        \n",
    "    X = data.drop(columns=\"linked_id\")\n",
    "    y = data[\"linked_id\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_split, shuffle=shuffle, random_state =seed)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def recall_at_k(resultTable : pd.DataFrame, trainingData: pd.DataFrame, testingData: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of K predictions for each query, first retrieve the correct ID from the test data,\n",
    "    then look in the training data the percentage of records that have been successfully identified.\n",
    "    \n",
    "    For example, given query \"1234-M\", first retrieve the correct ID \"1234\" from the test data,\n",
    "    then obtain from the training data all records that refer to \"1234\", \n",
    "    and finally look how many of them we have found;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain all the predictions for each record in the test set;\n",
    "    perQueryRecords = resultTable.groupby(\"queried_record_id\")\n",
    "    \n",
    "    # Group training records by their LinkedID truth value;\n",
    "    groupedTrainingRecords = trainingData.groupby(\"linked_id\")\n",
    "\n",
    "    totalRecall = 0.0\n",
    "\n",
    "    allRecords = dict()\n",
    "    \n",
    "    start = time.time()\n",
    "    for i, (queriedRecordID, group) in enumerate(perQueryRecords):\n",
    "        #if i % 1000 == 0 and i > 0:\n",
    "            #print(f\"processed {i}/{len(perQueryRecords)} records, {100 * i / len(perQueryRecords):.2f}%\")\n",
    "            #print(f\"\\tcurrent recall: {(totalRecall / i):.2f}\")\n",
    "            #print(f\"\\ttime elapsed: {(time.time() - start):.2f} s\")\n",
    "        \n",
    "        try:\n",
    "            queriedLinkedID = testingData.loc[queriedRecordID, \"linked_id\"]\n",
    "        except IndexError:\n",
    "            raise IndexError(\"ID {queriedRecordID} not found in testing data!\")\n",
    "        \n",
    "        try:\n",
    "            allRelevantRecords = set(groupedTrainingRecords.get_group(queriedLinkedID).index.values)\n",
    "        except KeyError:\n",
    "            allRelevantRecords = set()\n",
    "        setPredictedRecords = set(group[\"predicted_record_id\"])\n",
    "        selectedRelevantRecords = setPredictedRecords.intersection(allRelevantRecords)\n",
    "        recall = 1\n",
    "        if (len(allRelevantRecords) > 0):\n",
    "            recall = len(selectedRelevantRecords) / len(allRelevantRecords)\n",
    "\n",
    "        totalRecall += recall\n",
    "        allRecords[queriedRecordID] = [queriedRecordID, recall, len(selectedRelevantRecords), len(allRelevantRecords)]\n",
    "    \n",
    "    # Store the results in a summary table;\n",
    "    result_table =  pd.DataFrame.from_dict(\n",
    "                        allRecords,\n",
    "                        orient='index',\n",
    "                        columns=[\"QueriedRecordID\", \"Recall@K\", \"SelectedRecords\", \"AllRelevantRecords\"]\n",
    "                    )\n",
    "    # Compute the filtered recall, which considers only queries with at least one relevant record in the training data;\n",
    "    queries_with_relevant_records = result_table[result_table[\"AllRelevantRecords\"] > 0]\n",
    "    filtered_recall = np.mean(queries_with_relevant_records[\"SelectedRecords\"] / queries_with_relevant_records[\"AllRelevantRecords\"])\n",
    "\n",
    "    return {\n",
    "            \"AverageRecall\" : totalRecall / len(perQueryRecords),\n",
    "            \"AverageFilteredRecall\": filtered_recall,\n",
    "            \"perQueryResult\" : result_table\n",
    "            }\n",
    "    \n",
    "def precision_at_k(resultTable : pd.DataFrame, trainingData: pd.DataFrame, testingData: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of K predictions for each query, first retrieve the correct ID from the test data,\n",
    "    then look in the training data the percentage of records that are actually relevant;\n",
    "    \n",
    "    For example, given query \"1234-M\", first retrieve the correct ID \"1234\" from the test data,\n",
    "    then obtain from the training data all records that refer to \"1234\", \n",
    "    and finally look how many of the records we have found are actually referring to \"1234\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain all the predictions for each record in the test set;\n",
    "    perQueryRecords = resultTable.groupby(\"queried_record_id\")\n",
    "    \n",
    "    # Group training records by their LinkedID truth value;\n",
    "    groupedTrainingRecords = trainingData.groupby(\"linked_id\")\n",
    "\n",
    "    totalPrecision = 0.0\n",
    "    numberOfPredictionsForRelevantRecords = 0\n",
    "\n",
    "    allRecords = dict()\n",
    "    \n",
    "    start = time.time()\n",
    "    for i, (queriedRecordID, group) in enumerate(perQueryRecords):\n",
    "        #if i % 1000 == 0 and i > 0:\n",
    "            #print(f\"processed {i}/{len(perQueryRecords)} records, {100 * i / len(perQueryRecords):.2f}%\")\n",
    "            #print(f\"\\tcurrent precision: {(totalPrecision / i):.2f}\")\n",
    "            #print(f\"\\ttime elapsed: {(time.time() - start):.2f} s\")\n",
    "        \n",
    "        try:\n",
    "            queriedLinkedID = testingData.loc[queriedRecordID, \"linked_id\"]\n",
    "        except IndexError:\n",
    "            raise IndexError(\"ID {queriedRecordID} not found in testing data!\")\n",
    "        \n",
    "        try:\n",
    "            allRelevantRecords = set(groupedTrainingRecords.get_group(queriedLinkedID).index.values)\n",
    "        except KeyError:\n",
    "            allRelevantRecords = set()\n",
    "        setPredictedRecords = set(group[\"predicted_record_id\"])\n",
    "        selectedRelevantRecords = setPredictedRecords.intersection(allRelevantRecords)\n",
    "        precision = 1\n",
    "        if (len(allRelevantRecords) > 0):\n",
    "            precision = len(selectedRelevantRecords) / len(setPredictedRecords)\n",
    "            numberOfPredictionsForRelevantRecords += len(setPredictedRecords)\n",
    "\n",
    "        totalPrecision += precision\n",
    "        allRecords[queriedRecordID] = [queriedRecordID, precision, len(selectedRelevantRecords), len(allRelevantRecords)]\n",
    "    \n",
    "    # Store the results in a summary table;\n",
    "    result_table =  pd.DataFrame.from_dict(\n",
    "                        allRecords,\n",
    "                        orient='index',\n",
    "                        columns=[\"QueriedRecordID\", \"Precision@K\", \"SelectedRecords\", \"AllRelevantRecords\"]\n",
    "                    )\n",
    "    # Compute the filtered recall, which considers only queries with at least one relevant record in the training data;\n",
    "    queries_with_relevant_records = result_table[result_table[\"AllRelevantRecords\"] > 0]\n",
    "    filtered_precision = np.mean(queries_with_relevant_records[\"SelectedRecords\"] / numberOfPredictionsForRelevantRecords)\n",
    "\n",
    "    return {\n",
    "            \"AveragePrecision\" : totalPrecision / len(perQueryRecords),\n",
    "            \"AverageFilteredPrecision\": filtered_precision,\n",
    "            \"perQueryResult\" : result_table\n",
    "            }   \n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@timeit\n",
    "def prediction_dict_to_df(predictions: dict) -> pd.DataFrame:\n",
    "    # Turn the prediction dict into a series of tuples;\n",
    "    results = []\n",
    "    for query_id, pred_list in predictions.items():\n",
    "        for p in pred_list:\n",
    "            results += [[query_id, p]]\n",
    "    return pd.DataFrame(results, columns=[\"queried_record_id\", \"predicted_record_id\"])\n",
    "\n",
    "@timeit\n",
    "def prediction_dict_to_kaggle_df(predictions: dict) -> pd.DataFrame:\n",
    "    # Turn the prediction dict into a series of tuples;\n",
    "    results = []\n",
    "    for query_id, pred_list in predictions.items():\n",
    "        results += [[query_id, \" \".join(pred_list)]]\n",
    "    return pd.DataFrame(results, columns=[\"queried_record_id\", \"predicted_record_id\"])\n",
    "\n",
    "\n",
    "# A class for matching one list of strings to another\n",
    "#@ray.remote\n",
    "class StringMatch():\n",
    "    \n",
    "    def __init__(self,source_name_id,target_name_id,source_names,target_names,source_ad_id,target_ad_id,source_addresses,target_addresses,source_em_id,target_em_id,source_emails,target_emails,source_nu_id,target_nu_id,source_numbers,target_numbers):\n",
    "        \n",
    "        #self.target_link_ids=target_linked_ids\n",
    "        #self.source_record_ids=source_ids\n",
    "        #self.target_record_ids=target_ids\n",
    "        \n",
    "        \n",
    "        self.source_name_id=source_name_id\n",
    "        self.target_name_id=target_name_id\n",
    "        self.source_names = source_names\n",
    "        self.target_names = target_names\n",
    "        self.ct_vect      = None\n",
    "        self.tfidf_vect   = None\n",
    "        self.vocab        = None\n",
    "        self.sprse_mtx    = None\n",
    "        \n",
    "        \n",
    "        self.source_ad_id=source_ad_id\n",
    "        self.target_ad_id=target_ad_id\n",
    "        self.source_addresses = source_addresses\n",
    "        self.target_addresses = target_addresses\n",
    "        self.ct_vect_ad      = None\n",
    "        self.tfidf_vect_ad  = None\n",
    "        self.vocab_ad        = None\n",
    "        self.sprse_mtx_ad   = None\n",
    "        \n",
    "        \n",
    "        self.source_em_id=source_em_id\n",
    "        self.target_em_id=target_em_id\n",
    "        self.source_emails = source_emails\n",
    "        self.target_emails = target_emails\n",
    "        self.ct_vect_em      = None\n",
    "        self.tfidf_vect_em  = None\n",
    "        self.vocab_em        = None\n",
    "        self.sprse_mtx_em   = None\n",
    "        \n",
    "\n",
    "        self.source_nu_id=source_nu_id\n",
    "        self.target_nu_id=target_nu_id\n",
    "        self.source_numbers = source_numbers\n",
    "        self.target_numbers = target_numbers\n",
    "        self.ct_vect_nu      = None\n",
    "        self.tfidf_vect_nu  = None\n",
    "        self.vocab_nu        = None\n",
    "        self.sprse_mtx_nu   = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def tokenize(self, analyzer='char_wb', n=3):\n",
    "        '''\n",
    "        Tokenizes the list of strings, based on the selected analyzer\n",
    "        :param str analyzer: Type of analyzer ('char_wb', 'word'). Default is trigram\n",
    "        :param str n: If using n-gram analyzer, the gram length\n",
    "        '''\n",
    "        # Create initial count vectorizer & fit it on both lists to get vocab\n",
    "        self.ct_vect = CountVectorizer(analyzer=analyzer, ngram_range=(n-1, n))\n",
    "        self.vocab   = self.ct_vect.fit(self.source_names  + self.target_names).vocabulary_\n",
    "    \n",
    "        self.ct_vect_ad = CountVectorizer(analyzer=analyzer, ngram_range=(n-1, n))\n",
    "        self.vocab_ad   = self.ct_vect_ad.fit(self.source_addresses + self.target_addresses).vocabulary_\n",
    "        \n",
    "        \n",
    "        self.ct_vect_em = CountVectorizer(analyzer=analyzer, ngram_range=(n-1, n))\n",
    "        self.vocab_em   = self.ct_vect_em.fit(self.source_emails + self.target_emails).vocabulary_\n",
    "        \n",
    "        self.ct_vect_nu = CountVectorizer(analyzer=analyzer, ngram_range=(n-1, n))\n",
    "        self.vocab_nu   = self.ct_vect_nu.fit(self.source_numbers + self.target_numbers).vocabulary_\n",
    "        \n",
    "        # Create tf-idf vectorizer\n",
    "        self.tfidf_vect  = TfidfVectorizer(vocabulary=self.vocab, analyzer=analyzer, ngram_range=(n-1, n))\n",
    "        self.tfidf_vect_ad  = TfidfVectorizer(vocabulary=self.vocab_ad, analyzer=analyzer, ngram_range=(n-1, n)) \n",
    "        self.tfidf_vect_em  = TfidfVectorizer(vocabulary=self.vocab_em, analyzer=analyzer, ngram_range=(n-1, n))\n",
    "        self.tfidf_vect_nu  = TfidfVectorizer(vocabulary=self.vocab_nu, analyzer=analyzer, ngram_range=(n-1, n))\n",
    "        \n",
    "    def match(self, ntop=500, lower_bound=0, output_fmt='dict'):\n",
    "        '''\n",
    "        Main match function. Default settings return only the top candidate for every source string.\n",
    "        \n",
    "        :param int ntop: The number of top-n candidates that should be returned\n",
    "        :param float lower_bound: The lower-bound threshold for keeping a candidate, between 0-1.\n",
    "                                   Default set to 0, so consider all canidates\n",
    "        :param str output_fmt: The output format. Either dataframe ('df') or dict ('dict')\n",
    "        '''\n",
    "        self._awesome_cossim_top(ntop,lower_bound,use_threads=True,n_jobs=20)\n",
    "        \n",
    "        if output_fmt == 'df':\n",
    "            \n",
    "            match_output = self._make_matchdf()\n",
    "           \n",
    "            \n",
    "        elif output_fmt == 'dict':\n",
    "            \n",
    "            match_output_name,match_output_ad,match_output_em,match_output_nu = self._make_matchdict()\n",
    "            \n",
    "            \n",
    "        return match_output_name,match_output_ad,match_output_em,match_output_nu\n",
    "        \n",
    "        \n",
    "    def _awesome_cossim_top(self, ntop, lower_bound,use_threads=True,n_jobs=20):\n",
    "        ''' https://gist.github.com/ymwdalex/5c363ddc1af447a9ff0b58ba14828fd6#file-awesome_sparse_dot_top-py '''\n",
    "        # To CSR Matrix, if needed\n",
    "        A = self.tfidf_vect.fit_transform(self.source_names).tocsr()\n",
    "        B = self.tfidf_vect.fit_transform(self.target_names).transpose().tocsr()\n",
    "        M, _ = A.shape\n",
    "        _, N = B.shape\n",
    "        \n",
    "        \n",
    "        idx_dtype = np.int32\n",
    "\n",
    "        nnz_max = M * ntop\n",
    "\n",
    "        indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "        indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "        data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "        ct_thread.sparse_dot_topn_threaded(\n",
    "            M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "            np.asarray(A.indices, dtype=idx_dtype),\n",
    "            A.data,\n",
    "            np.asarray(B.indptr, dtype=idx_dtype),\n",
    "            np.asarray(B.indices, dtype=idx_dtype),\n",
    "            B.data,\n",
    "            ntop,\n",
    "            lower_bound,\n",
    "            indptr, indices, data, n_jobs)\n",
    "\n",
    "        self.sprse_mtx = csr_matrix((data,indices,indptr), shape=(M,N))\n",
    "        \n",
    "        \n",
    "        C = self.tfidf_vect_ad.fit_transform(self.source_addresses).tocsr()\n",
    "        D = self.tfidf_vect_ad.fit_transform(self.target_addresses).transpose().tocsr()\n",
    "        M_C, _ = C.shape\n",
    "        _, N_D = D.shape\n",
    "        \n",
    "        \n",
    "        nnz_max = M_C * ntop\n",
    "        indptr = np.zeros(M_C+1, dtype=idx_dtype)\n",
    "        indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "        data = np.zeros(nnz_max, dtype=C.dtype)\n",
    "        \n",
    "        ct_thread.sparse_dot_topn_threaded(\n",
    "            M_C, N_D, np.asarray(C.indptr, dtype=idx_dtype),\n",
    "            np.asarray(C.indices, dtype=idx_dtype),\n",
    "            C.data,\n",
    "            np.asarray(D.indptr, dtype=idx_dtype),\n",
    "            np.asarray(D.indices, dtype=idx_dtype),\n",
    "            D.data,\n",
    "            ntop,\n",
    "            lower_bound,\n",
    "            indptr, indices, data, n_jobs)\n",
    "        \n",
    "        self.sprse_mtx_ad= csr_matrix((data,indices,indptr), shape=(M_C,N_D))\n",
    "        \n",
    "        \n",
    "        E = self.tfidf_vect_em.fit_transform(self.source_emails).tocsr()\n",
    "        F = self.tfidf_vect_em.fit_transform(self.target_emails).transpose().tocsr()\n",
    "        M_E, _ = E.shape\n",
    "        _, N_F = F.shape\n",
    "        \n",
    "        \n",
    "        nnz_max = M_E * ntop\n",
    "        indptr = np.zeros(M_E+1, dtype=idx_dtype)\n",
    "        indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "        data = np.zeros(nnz_max, dtype=E.dtype)\n",
    "        \n",
    "        ct_thread.sparse_dot_topn_threaded(\n",
    "            M_E, N_F, np.asarray(E.indptr, dtype=idx_dtype),\n",
    "            np.asarray(E.indices, dtype=idx_dtype),\n",
    "            E.data,\n",
    "            np.asarray(F.indptr, dtype=idx_dtype),\n",
    "            np.asarray(F.indices, dtype=idx_dtype),\n",
    "            F.data,\n",
    "            ntop,\n",
    "            lower_bound,\n",
    "            indptr, indices, data,n_jobs)\n",
    "        \n",
    "        self.sprse_mtx_em= csr_matrix((data,indices,indptr), shape=(M_E,N_F))\n",
    "        \n",
    "        \n",
    "        G = self.tfidf_vect_nu.fit_transform(self.source_numbers).tocsr()\n",
    "        H = self.tfidf_vect_nu.fit_transform(self.target_numbers).transpose().tocsr()\n",
    "        M_G, _ = G.shape\n",
    "        _, N_H = H.shape\n",
    "        \n",
    "        nnz_max = M_G * ntop\n",
    "        indptr = np.zeros(M_G+1, dtype=idx_dtype)\n",
    "        indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "        data = np.zeros(nnz_max, dtype=G.dtype)\n",
    "        \n",
    "        ct_thread.sparse_dot_topn_threaded(\n",
    "            M_G, N_H, np.asarray(G.indptr, dtype=idx_dtype),\n",
    "            np.asarray(G.indices, dtype=idx_dtype),\n",
    "            G.data,\n",
    "            np.asarray(H.indptr, dtype=idx_dtype),\n",
    "            np.asarray(H.indices, dtype=idx_dtype),\n",
    "            H.data,\n",
    "            ntop,\n",
    "            lower_bound,\n",
    "            indptr, indices, data, n_jobs)\n",
    "        \n",
    "        self.sprse_mtx_nu= csr_matrix((data,indices,indptr), shape=(M_G,N_H))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _make_matchdf(self):\n",
    "        ''' Build dataframe for result return '''\n",
    "        # CSR matrix -> COO matrix\n",
    "        cx = self.sprse_mtx.tocoo()\n",
    "\n",
    "        # COO matrix to list of tuples\n",
    "        #match_list_us = [[] for i in range(len(self.source_record_ids))]\n",
    "        match_list=[]\n",
    "        for row,col,val in zip(cx.row, cx.col, cx.data):\n",
    "            match_list.append((row, self.source_names[row], col, self.target_names[col], val))\n",
    "            \n",
    "        # List of tuples to dataframe\n",
    "        colnames = ['Row Idx', 'Title', 'Candidate Idx', 'Candidate Title', 'Score']\n",
    "        match_df = pd.DataFrame(match_list, columns=colnames)\n",
    "\n",
    "        return match_df\n",
    "\n",
    "    \n",
    "    def _make_matchdict(self):\n",
    "        ''' Build dictionary for result return '''\n",
    "        # CSR matrix -> COO matrix\n",
    "        cx_name = self.sprse_mtx.tocoo()\n",
    "\n",
    "        # dict value should be tuple of values\n",
    "        match_dict_name = {}\n",
    "        for row,col,val in zip(cx_name.row, cx_name.col, cx_name.data):\n",
    "            if match_dict_name.get(self.source_name_id[row]):\n",
    "                match_dict_name[self.source_name_id[row]].append(self.target_name_id[col])\n",
    "            else:\n",
    "                match_dict_name[self.source_name_id[row]] = [self.target_name_id[col]]\n",
    "\n",
    "        \n",
    "        \n",
    "        cx_ad = self.sprse_mtx_ad.tocoo()\n",
    "        match_dict_ad = {}\n",
    "        for row,col,val in zip(cx_ad.row, cx_ad.col, cx_ad.data):\n",
    "            if match_dict_ad.get(self.source_ad_id[row]):\n",
    "                match_dict_ad[self.source_ad_id[row]].append(self.target_ad_id[col])\n",
    "            else:\n",
    "                match_dict_ad[self.source_ad_id[row]] = [self.target_ad_id[col]]\n",
    "        \n",
    "        \n",
    "        cx_em= self.sprse_mtx_em.tocoo()\n",
    "        match_dict_em = {}\n",
    "        for row,col,val in zip(cx_em.row, cx_em.col, cx_em.data):\n",
    "            if match_dict_em.get(self.source_em_id[row]):\n",
    "                match_dict_em[self.source_em_id[row]].append(self.target_em_id[col])\n",
    "            else:\n",
    "                match_dict_em[self.source_em_id[row]] = [self.target_em_id[col]]\n",
    "        \n",
    "        \n",
    "        cx_nu= self.sprse_mtx_nu.tocoo()\n",
    "        match_dict_nu = {}\n",
    "        for row,col,val in zip(cx_nu.row, cx_nu.col, cx_nu.data):\n",
    "            if match_dict_nu.get(self.source_nu_id[row]):\n",
    "                match_dict_nu[self.source_nu_id[row]].append(self.target_nu_id[col])\n",
    "            else:\n",
    "                match_dict_nu[self.source_nu_id[row]] = [self.target_nu_id[col]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return match_dict_name,match_dict_ad,match_dict_em,match_dict_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "train=pd.read_csv('../panama-papers-polimi/data/panama_train_expanded_2.csv')\n",
    "test=pd.read_csv('../panama-papers-polimi/data/panama_test_expanded_2.csv')\n",
    "\n",
    "\n",
    "training_file = \"../panama-papers-polimi/data/entity-resolution_advanced-topics-training_data.csv\"\n",
    "train_or = read_file(training_file, set_record_id_as_index=False)\n",
    "        \n",
    "\n",
    "y_train_or = train_or[[\"linked_id\"]]\n",
    "train.phone=train.phone.apply(str)\n",
    "test.phone=test.phone.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.fillna('-1')\n",
    "test=test.fillna('-1')\n",
    "\n",
    "train = pd.concat([train,y_train_or['linked_id']],axis=1)\n",
    "\n",
    "\n",
    "train = train[['name','record_id','address','email','phone','linked_id']]\n",
    "test = test[['name','record_id','address','email','phone']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treat strings that are too short as missing values\n",
    "train.name=train.name.apply(lambda x : '-1' if len(x)<=2 else x)\n",
    "train.address=train.address.apply(lambda x : '-1' if len(x)<=2 else x)\n",
    "train.email=train.email.apply(lambda x : '-1' if len(x)<=2 else x)\n",
    "train.phone=train.phone.apply(lambda x : '-1' if len(x)<=2 else x)\n",
    "\n",
    "test.name=test.name.apply(lambda x : '-1' if len(x)<=2 else x)\n",
    "test.address=test.address.apply(lambda x : '-1' if len(x)<=2 else x)\n",
    "test.email=test.email.apply(lambda x : '-1' if len(x)<=2 else x)\n",
    "test.phone=test.phone.apply(lambda x : '-1' if len(x)<=2 else x)\n",
    "\n",
    "train=train.fillna('-1')\n",
    "test=test.fillna('-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of valid features\n",
    "test_name_id=test.record_id[test.name!='-1']\n",
    "test_ad_id=test.record_id[test.address!='-1']\n",
    "test_em_id=test.record_id[test.email!='-1']\n",
    "test_nu_id=test.record_id[test.phone!='-1']\n",
    "\n",
    "train_name_id=train.record_id[train.name!='-1']\n",
    "train_ad_id=train.record_id[train.address!='-1']\n",
    "train_em_id=train.record_id[train.email!='-1']\n",
    "train_nu_id=train.record_id[train.phone!='-1']\n",
    "\n",
    "test_name_id=test_name_id.tolist()\n",
    "test_ad_id=test_ad_id.tolist()\n",
    "test_em_id=test_em_id.tolist()\n",
    "test_nu_id=test_nu_id.tolist()\n",
    "\n",
    "train_name_id=train_name_id.tolist()\n",
    "train_ad_id=train_ad_id.tolist()\n",
    "train_em_id=train_em_id.tolist()\n",
    "train_nu_id=train_nu_id.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_name_list=test.name[test.name!='-1']\n",
    "test_address_list=test.address[test.address!='-1']\n",
    "test_email_list=test.email[test.email!='-1']\n",
    "test_number_list=test.phone[test.phone!='-1']\n",
    "\n",
    "\n",
    "train_name_list=train.name[train.name!='-1']\n",
    "train_address_list=train.address[train.address!='-1']\n",
    "train_email_list=train.email[train.email!='-1']\n",
    "train_number_list=train.phone[train.phone!='-1']\n",
    "\n",
    "\n",
    "\n",
    "train_name_list = train_name_list.tolist()\n",
    "train_address_list = train_address_list.tolist()\n",
    "train_email_list=train_email_list.tolist()\n",
    "train_number_list=train_number_list.tolist()\n",
    "\n",
    "\n",
    "\n",
    "test_name_list = test_name_list.tolist()\n",
    "test_address_list = test_address_list.tolist()\n",
    "test_email_list=test_email_list.tolist()\n",
    "test_number_list=test_number_list.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inizialization of tfidf model used to create small blocks\n",
    "best_matches = StringMatch(test_name_id,train_name_id,test_name_list,train_name_list,test_ad_id,train_ad_id,test_address_list,train_address_list,test_em_id,train_em_id,test_email_list,train_email_list,test_nu_id,train_nu_id,test_number_list,train_number_list)\n",
    "#Tokenization and creation of tfidf matrices\n",
    "best_matches.tokenize()\n",
    "#Sparse dot multiplication\n",
    "match_output_name,match_output_ad,match_output_em,match_output_nu= best_matches.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create unique dict of good candidates from the 4 matrices similarity scores\n",
    "d = {}\n",
    "for key in set(list(match_output_name.keys()) + list(match_output_ad.keys()) + list(match_output_em.keys()) + list(match_output_nu.keys())):\n",
    "    try:\n",
    "        d.setdefault(key,[]).append(match_output_name[key])        \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        d.setdefault(key,[]).append(match_output_ad[key])          \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    try:\n",
    "        d.setdefault(key,[]).append(match_output_em[key])        \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        d.setdefault(key,[]).append(match_output_nu[key])          \n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "for key,value in d.items():\n",
    "    d[key]=[x for b in d[key] for x in b]\n",
    "\n",
    "#remove duplicates \n",
    "for key,value in d.items():\n",
    "    d[key]=list(set(d[key]))\n",
    "\n",
    "\n",
    "for key,value in d.items():\n",
    "    d[key]=[t for t in d[key] if type(t)==str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframes to evaluate performance with Recall and Precision as metrics\n",
    "train_2=train.copy()\n",
    "test_2=test.copy()\n",
    "train_2 = train_2[['name','record_id','address','email','phone']]\n",
    "test_2 = test_2[['name','record_id','address','email','phone']]\n",
    "test_2['linked_id']=test_2['record_id']\n",
    "test_2.set_index('record_id',inplace=True)\n",
    "train_2.set_index('record_id',inplace=True)\n",
    "\n",
    "y_test=test_2[['linked_id']]\n",
    "y_test.linked_id=y_test.linked_id.apply(lambda x : x.split('-')[0])\n",
    "train.set_index('record_id',inplace=True)\n",
    "y_train=train[['linked_id']]\n",
    "\n",
    "test_3=test.copy()\n",
    "test_3=test_3[['name','record_id','address','email','phone']]\n",
    "test_3.set_index('record_id',inplace=True)\n",
    "train_m = train_2.merge(y_train, left_index=True, right_index=True)\n",
    "test_m = test_3.merge(y_test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new merged features from name,address,phone,email\n",
    "train_copy=train_copy.fillna('')\n",
    "test=test.fillna('') \n",
    "train_copy.name=train_copy.name.apply(lambda x : '' if x=='-1' else x)\n",
    "train_copy.address=train_copy.address.apply(lambda x : '' if x=='-1' else x)\n",
    "train_copy.email=train_copy.email.apply(lambda x : '' if x=='-1' else x)\n",
    "train_copy.phone=train_copy.phone.apply(lambda x : '' if x=='-1' else x)\n",
    "\n",
    "test.name=test.name.apply(lambda x : '' if x=='-1' else x)\n",
    "test.address=test.address.apply(lambda x : '' if x=='-1' else x)\n",
    "test.email=test.email.apply(lambda x : '' if x=='-1' else x)\n",
    "test.phone=test.phone.apply(lambda x : '' if x=='-1' else x)\n",
    "\n",
    "train_copy=train_copy[['record_id','name','address','email','phone']]\n",
    "train_copy['complessivo']=train_copy['name']+' '+train_copy['address']+' '+train_copy['email']+' '+train_copy['phone']\n",
    "train_copy.set_index('record_id',inplace=True)\n",
    "\n",
    "test['complessivo']=test['name']+' '+test['address']+' '+test['email']+' '+test['phone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=0\n",
    "K=10\n",
    "tot=10\n",
    "match_df={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorized fuzzy string matching operation in order to calculate all the similarity scores for one test instance in just one step\n",
    "def partial_match_totale(x,y):\n",
    "    return(fuzz.partial_token_set_ratio(x,y)*0.1)\n",
    "partial_match_vector_totale = np.vectorize(partial_match_totale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use small blocks created with tfidf to perform fast fuzzy matching\n",
    "\n",
    "for row_i in zip(test['record_id'],test['complessivo']):\n",
    "    row_i_vector = np.array(row_i[1:])\n",
    "    print(cc)\n",
    "    if not d.get(row_i[0]):\n",
    "        print(row_i[0])\n",
    "        match_df[row_i[0]]=[]\n",
    "        cc+=1\n",
    "        continue\n",
    "\n",
    "    identical_values_per_row=pd.Series([0]*len(d[row_i[0]]))\n",
    "    identical_values_per_row.index=train_copy.loc[d[row_i[0]],'complessivo'].index\n",
    "\n",
    "    #Fuzzy match with features concatenated\n",
    "    df0 = pd.DataFrame([row_i_vector[0]])\n",
    "    df0.columns = ['Match0']\n",
    "\n",
    "    compare0 = pd.DataFrame(train_copy.loc[d[row_i[0]],'complessivo'])\n",
    "    compare0.columns = ['compare0']\n",
    "\n",
    "    df0['Key0'] = 1\n",
    "    compare0['Key0'] = 1\n",
    "    combined_dataframe0 = df0.merge(compare0,on=\"Key0\",how=\"left\")\n",
    "\n",
    "\n",
    "    identical_values_per_row += partial_match_vector_totale(combined_dataframe0['Match0'],combined_dataframe0['compare0'])\n",
    "    best_matches = identical_values_per_row.sort_values(ascending=False)[:10]\n",
    "    match_df[row_i[0]]=list(y_train.loc[best_matches.index.values]['linked_id'].values)\n",
    "    cc+=1\n",
    "    if cc % 1000 == 0 and cc > 0:\n",
    "        print(match_df[row_i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create kaggle format predictions and calculate metrics\n",
    "\n",
    "pred_df_kaggle = prediction_dict_to_kaggle_df(match_df)\n",
    "pred_df_kaggle.to_csv(\"../panama-papers-polimi/preds/kaggle_fuzzy_partial_token_set_top_10\"+'.csv', index=False)\n",
    "pred_df = prediction_dict_to_df(match_df)\n",
    "\n",
    "#%% 5. Compute recall@K;\n",
    "recall_dict = recall_at_k(pred_df, train_m, test_m)\n",
    "print(recall_dict[\"AverageRecall\"])\n",
    "print(recall_dict[\"AverageFilteredRecall\"])\n",
    "\n",
    "#%% 6. Compute MAP@K;\n",
    "precision_dict = precision_at_k(pred_df, train_m, test_m)\n",
    "print(precision_dict[\"AveragePrecision\"])\n",
    "print(precision_dict[\"AverageFilteredPrecision\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
